Detokenizer Implementation Status
.................................

    Author :: Arthur Debert
    Date :: 2025-01-13
    Status :: Mostly Complete

The detokenizer is now functional and passes 50 out of 52 tests. It successfully
handles round-trip verification for most txxt constructs.

## Implementation Details

The detokenizer takes tokens produced by the lexer and reconstructs source text
that, when re-tokenized, produces identical tokens. This is crucial for:
- Verifying parsing pipeline correctness
- Debugging tokenization issues  
- Round-trip testing

Key design decisions:
- With the introduction of explicit Whitespace tokens (issue #24), the 
  detokenizer no longer needs heuristic spacing logic
- Indent/Dedent tokens track level changes, not actual whitespace
- VerbatimTitle and VerbatimContent tokens require explicit newlines

## Test Results

Current status: 50 passing, 2 failing

The two failing tests are blocked by issue #27:
- test_nested_lists
- test_deeply_nested

Both failures occur because indented sequence markers (like "    - Item") are
not recognized as SequenceMarker tokens by the tokenizer.

## Next Steps

1. Fix issue #27 (indented sequence markers bug)
2. Create txxt-detokenize executable for command-line usage
3. Add more complex test cases using docs/walkthrough.txxt
4. Consider adding pretty-printing options (canonical formatting)