Proposal: Semantic Token Stage

 1. Context & Motivation

    The current txxt parsing pipeline builds a `TokenTree` early in the process (Phase 1c). While this captures the document's hierarchy, it consumes the `Indent` and `Dedent` tokens prematurely. The subsequent block parser (Phase 2a) then operates on this tree, forcing it to infer complex container rules without the explicit indentation markers that define them. This makes the parser complex and negates the benefits of having a simple, declarative grammar based on indentation.

    This proposal refactors the pipeline into a Stream-First design. The core principle is to *delay tree formation until the final step*. By maintaining a flat stream of tokens through the semantic analysis phase, we provide the block parser with all the information it needs—both high-level semantic nodes and structural indentation markers—to build the AST directly and efficiently.
	Currently txxt parsing happens as follows: 

	Phase 1: Lexer (txxt str -> TokenTree)
		a. Verbatim Scanner: marks verbatim lines that are off-limits for processing as txxt
		b. Token List: creates the token stream at low level tokens -> token list
		c. Token Tree: converts the token list to a tree, using indent/dedent tokens -> TokenTree
	Phase 2: Parser (TokenTree -> AST tree node)
		a. Block Parsing: Convert TokenTrees into typed AST nodes -> AST node (no inlines)
		b. Inline Parsing: Handle inlines within blocks -> AST node
	Phase 3: Assembly (AST tree node -> AST document node)
		a. Document Wrapping: wraps the parsed AST in a Document node.
		b. Annotations Attachments: from the content tree to node's annotation's field.

	While Phase 2, the block parsing is underway. The parser operates on low level tokens (as in <PERIOD>, <WHITESPACE>), which makes the mental mapping from low level tokens to full AST elements challenging and indirect.

2. Pipeline Changes

	In this design, we do away with the early token stream -> token tree convertion. 
	Then the semantic token parsing outputs the semantic tokens with <indent> and <detent> tokens unchanged.

	On the actual block parsing, we can use the direct mapping grammar where Semantic Tokens + Indentation Tokens can easily be parsed as Elements. 
	In this layer, the indent token gets consumed by a new element , and the parser keeps tabs of dedentes to actually build a tree.


	Here is what this looks Like

	Phase 1: Lexer: Operates On Strings (txxt str -> ScannerTokenList)
		a. Verbatim Scanner: marks verbatim lines that are off-limits for processing as txxt
		b. Token List: creates the token stream at low level tokens -> scanner token list
	Phase 2: Parser Operates on Tokens (ScannerTokenList -> AST tree node)
		a. Semantic Analysis  (ScannerTokenList → SemanticTokenList)
			 Elevates the low-level scanner token stream into a higher-level stream of semantic nodes, which represent the syntactic structure of individual lines.  This stage consumes the flat `ScannerToken` stream. It groups tokens on a line-by-line basis into the `SemanticNode` types defined in [docs/specs/core/semantic-nodes.txxt].
            	- Example: `SequenceMarker`, `Whitespace`, `Text`, `Newline` are grouped into a single `SequenceLine` node.
            	- Crucially, structural tokens like `Indent`, `Dedent`, and `BlankLine` are *passed through unchanged*.
        	Output Stream Example:
            [AnnotationLine, Indent, SequenceLine, SequenceLine, Dedent, BlankLine, TextLine, ...]( Token List)
		b. AST Construction (SemanticTokenList -> Ast Tree Node)
        	This is the final and *only* phase that builds a tree. It consumes the rich, flat stream from Phase 2 and applies the grammar to construct the final AST.
        	This parser now has everything it needs to apply the grammar from [docs/specs/core/grammar.txxt] directly. It can be implemented as a simple recursive-descent or state-machine parser.
            - On seeing `TextLine` + `BlankLine` + `Indent`, it knows it's parsing a `Session` and can recursively call itself to parse the `SessionContainer`.
            - On seeing a `Dedent` token, it knows a container has closed and returns from the recursion.
		c. Inline Parsing: Handle inlines within blocks (ScannerToken -> AST node). In the text spans, inlines will still be in token form, hence if parses that into ast inline nodes.
	Phase 3: Assembly (AST tree node -> AST document node)
		a. Document Wrapping: wraps the parsed AST in a Document node.
		b. Annotations Attachments: from the content tree to node's annotation's field.

3. Semantic Tokens: 

	The conversion between scanner and Semantic Tokens is based on the syntax [./docs/specs/core/syntax.txxt] and aims to output a higher level representation that is much closer to the parser level. 

	Verbatim: 

		<text span> <colon>
			<ignore line>
		<label> <parameters>?
    :: syntax

	And for annotations: 
		<txxt marker> <label> <txxt-marker> <text-line> 
	:: syntax

 .  Note that semantic tokes are not element bound, as in annotation token of any sorts. We won't know about elements until parsing has taken place, which will come later. Semenatic tokes group lower level character tokens into higher level langauge concepts as parameters, label, sequence text line.

4. Semantic Token Types and Definitions

	4.1 Proposed SemanticToken AST Node Types

		SemanticToken::
			Component-level semantic tokens that bridge scanner tokens and AST elements.
			Provides structured, reusable components rather than line-based tokens.
			Emphasizes composability and eliminates duplication across elements.

		SemanticTokenTree::
			Tree structure containing SemanticTokens with hierarchical children.
			Mirrors TokenTree structure but with higher-level semantic meaning.

	4.2 Semantic Tokens Definitions

		These are describe in the semantic nodes [docs/specs/core/semantic-nodes.txxt]

5. Implementation: 
 
	5.1 Renaming Tokens

		Now that we have two token types, having tokens vs Semantic Tokens is bound to create confusion. We will rename, in the AST, the current Tokens to ScannerTokens, as opposed to SemanticTokens.

		While these do share considerably in traits/types, they have specialization and being able to work with scanner or semantic trees with type safety will be worth it. 

		These should reflect code, be it modules or packages, as in ast/tokens.rs -> ast/scanner_tokens.rs


	5.2 Validate Semantic Token Definition List

		This should be validated, then formalized in the docs/specs/core/syntax.txxt document.

	5.3 Implement Infrastructure for Semantic Tokens

		The AST nodes, etc.

	5.4 Add SemanticTokenTree txxt-binary support

		Add the support for txxt binary to output these, as it's greatly useful for debugging.

	5.5 Add Semantic Token support for Corpora

		Our txxt source tool, corpora can generate the txxt data in all phases of the pipeline, which are useful for testing.

	5.6 Implement the Semantic Token parsing phase

	5.7 Write Unit Tests

		These should have a few tests per token. We can use the txxt binary or corpora to generate this correctly from actual parsing stages, but then we copy and use them manually in the tests, to make it more visual what the transformation of input/output should look like

	5.8 Don't break parsing as it is

		This branch will not adapt the block parser to receive Semantic Tokens. Hence we cannot just plug in the phase, else all will break. We will implement the phase, but not include it in the pipeline.