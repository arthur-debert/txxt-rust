:: title :: Detokenizer Remaining Issues
:: author :: Development Notes
:: date :: 2025-10-13

This document tracks the remaining issues found during detokenizer development.

1. Tokenizer: Escape Sequence Support (Critical)

    The tokenizer does not handle backslash escape sequences, which are required by the txxt specification.

    Problem ::
        Text like `\*not bold\*` should preserve the backslashes and not treat the asterisks as delimiters.
        Currently, the backslash is dropped and the asterisks become BoldDelimiter tokens.

    Impact ::
        - Round-trip tests fail for escaped characters
        - Users cannot display literal delimiter characters
        - Specification compliance issue

    Location ::
        src/tokenizer/infrastructure/lexer.rs - read_text() method needs escape handling

    Test Case ::
        test_escaping_special_chars in tests/detokenizer_walkthrough_tests.rs

2. Tokenizer: Blank Line Whitespace Preservation (Medium)

    BlankLine tokens do not preserve leading whitespace, causing reconstruction mismatches.

    Problem ::
        A blank line with indentation like "    \n" becomes just "\n" after round-trip.
        The tokenizer consumes the whitespace but doesn't preserve it in the BlankLine token.

    Impact ::
        - Token count mismatches in round-trip tests
        - Indented blank lines lose their indentation
        - Formatting preservation issue

    Test Case ::
        test_complex_document_structure in tests/detokenizer_walkthrough_tests.rs

3. Verbatim Scanner: False Positive Detection (Low)

    The verbatim scanner misidentifies some annotation patterns as verbatim blocks.

    Problem ::
        Lines ending with ":" followed by annotation markers can be misinterpreted.
        Example: "Annotations can have parameters:\n:: warning:severity=high ::"

    Workaround ::
        Use period instead of colon before annotation markers

4. Successfully Fixed Issues

    The following issues were identified and fixed:

    - Caret character (^) not tokenized - Fixed in lexer.rs
    - Parameter separators missing in detokenizer - Fixed for verbatim labels
    - Detokenizer now handles 50 of 52 original tests plus 10 of 12 complex tests