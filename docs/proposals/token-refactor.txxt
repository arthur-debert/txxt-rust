:: title :: Token Architecture Refactoring Proposal
:: author :: Arthur Debert 
:: date :: 2025-01-18

Critical refactoring proposal to fix fundamental architectural issues in the TXXT token design.

These are to be tackle before issue #65 , but this means that implementing semantic analysis is out of scope for this work.

1. Problem Analysis

    1.1. Core Issue: Semantic Decisions in Scanner

        The current scanner makes semantic interpretations too early, violating clean separation between lexical analysis (what characters exist) and semantic analysis (what those characters mean in context).

        Current problematic tokens:
        - AnnotationMarker: Scanner decides if :: label :: is an annotation
        - DefinitionMarker: Scanner decides if term :: is a definition  
        - VerbatimTitle: Scanner decides if text: is verbatim title
        - VerbatimLabel: Scanner decides if :: label is verbatim terminator
        - Parameter: Scanner parses key=value structures

        This approach:
        - Makes grammar unparseable (lost compositional rules)
        - Creates brittle context-dependent tokenization
        - Duplicates parameter handling across contexts
        - Prevents clean semantic token transformation

    1.2. Column Numbering and Disambiguation

        Current confusion about column numbering:
        - Code uses 0-based columns (Position.column comment confirms)
        - Stretched mode needs disambiguation from title level
        - Solution: Stretched content at absolute column 1 (0-based)
        - Ensures title at column N, stretched content at column 1 (N > 1)

    1.3. VerbatimContent Wall Problem

        Current VerbatimContent token loses critical structural information:
        - Scanner calculates indentation wall correctly
        - But bundles wall-stripped content into monolithic token
        - Loses ability to reconstruct original formatting
        - Prevents proper language server support

2. Proposed Token Architecture

    2.1. Scanner Tokens (Format-Only)

        2.1.1. Content Tokens
            - Text: Regular text content
            - Whitespace: Spaces and tabs (not newlines)
            - Identifier: Alphanumeric names for labels/parameters

        2.1.2. Structural Tokens  
            - Indent: Indentation increase
            - Dedent: Indentation decrease
            - BlankLine: Empty or whitespace-only lines
            - Newline: Line break characters
            - Eof: End of file marker

        2.1.3. Formatting Tokens
            - BoldDelimiter: * character
            - ItalicDelimiter: _ character  
            - CodeDelimiter: ` character
            - MathDelimiter: # character

        2.1.4. Punctuation Tokens
            - Dash: - character
            - Period: . character
            - LeftBracket: [ character
            - RightBracket: ] character
            - AtSign: @ character
            - LeftParen: ( character
            - RightParen: ) character
            - Colon: : character
            - Equals: = character
            - Comma: , character

        2.1.5. Marker Tokens
            - TxxtMarker: :: character sequence
            - SequenceMarker: List markers (1., -, a), etc.) with rich type info

        2.1.6. Reference Tokens (Context-Independent)
            - RefMarker: [text] patterns  
            - FootnoteRef: [1], [^label] patterns
            - CitationRef: [@key] patterns
            - PageRef: [p.123] patterns
            - SessionRef: [#1.2] patterns

        2.1.7. Verbatim Tokens (Structure-Preserving)
            - IndentationWall: Wall position marker with level
            - IgnoreTextSpan: Raw content after wall

    2.2. Semantic Tokens (Context-Aware Compositions)

        2.2.1. Structural Patterns
            - AnnotationPattern: TxxtMarker + Label + TxxtMarker
            - DefinitionPattern: TextSpan + TxxtMarker  
            - VerbatimTitlePattern: TextSpan + Colon
            - VerbatimLabelPattern: TxxtMarker + Identifier

        2.2.2. Parameter Composition
            - ParameterPattern: Identifier + Equals + Value + Comma?
            - ParametersGroup: Multiple ParameterPattern instances

        2.2.3. Content Patterns
            - VerbatimContentLine: IndentationWall + IgnoreTextSpan
            - SequenceTextLine: SequenceMarker + TextSpan
            - PlainTextLine: TextSpan only

3. Implementation Plan

	CRITICAL: No backwareds compatibility or flags, this is a hard switch over. This means at every phase there will be breaking changes.  It is at the core of the task to fix these, as in update test and callers , including the previous parser pipeline (which we will keep active until the semantic one is ready.)

    1. Phase 1: Scanner Token Cleanup
        a. Add Missing Punctuation Tokens
            - Add: Equals, Comma (for parameter parsing)
            - Move: TxxtMarker from semantic to scanner level
        b. Remove Semantic Scanner Tokens
            - Delete: AnnotationMarker, DefinitionMarker, VerbatimTitle, VerbatimLabel
            - Delete: Parameter (replace with Identifier + Equals + Value)
        c. Fix VerbatimContent Structure
            - Replace: VerbatimContent with IndentationWall + IgnoreTextSpan
            - Preserve: Wall position information for reconstruction
    2. Phase 2: Specification Updates
        a. Update Grammar Specification
            - File: docs/specs/core/grammar.txxt
            - Add: Compositional rules for all patterns
            - Remove: Monolithic semantic decisions
        b. Update Scanner Token Specification  
            - File: docs/specs/core/scanner-tokens.txxt
            - Focus: Format-only token definitions
            - Remove: Semantic interpretation responsibilities
        c. Update Semantic Token Specification
            - File: docs/specs/core/semantic-tokens.txxt  
            - Add: Composition rules and patterns
            - Define: Context-aware transformations

4. Verbatim Wall Architecture

    4.1. Wall Position Rules

        In-flow mode:
        - Wall position: title_indent + 4 spaces
        - Content: Relative to wall, preserves internal structure
        - Example: Title at column 8 ï¿½ wall at column 12

        Stretched mode:
        - Wall position: Absolute column 1 (0-based indexing)
        - Disambiguation: Always different from title column (title e column 4)
        - Content: Relative to wall, preserves full width

    4.2. Scanner Token Structure

        For verbatim content line "    def example():"
        
        In-flow mode (title at column 4):
            IndentationWall { level: 8, span: SourceSpan(4,8) }
            IgnoreTextSpan { content: "    def example():", span: SourceSpan(8,end) }

        Stretched mode:
            IndentationWall { level: 1, span: SourceSpan(0,1) }  
            IgnoreTextSpan { content: "    def example():", span: SourceSpan(1,end) }

    4.3. Reconstruction Support

        Original formatting reconstruction:
        - Wall position determines content placement
        - IgnoreTextSpan preserves exact content
        - Language server can provide precise positioning
        - Export tools can maintain formatting

5. Parameter Architecture

    5.1. Current Problem

        Scanner currently parses entire "key=value" as Parameter token, doing semantic work too early.

    5.2. Proposed Scanner Decomposition

        Input: "version=3.11,style=functional"
        
        Scanner tokens:
            Identifier { content: "version" }
            Equals { }
            Identifier { content: "3.11" }  # or Value type if different
            Comma { }
            Identifier { content: "style" }
            Equals { }
            Identifier { content: "functional" }

    5.3. Semantic Token Composition

        Semantic transformation:
            ParameterPattern { key: "version", value: "3.11" }
            ParameterPattern { key: "style", value: "functional" }
            
        AST parsing:
            Parameters { pairs: HashMap<String, String> }

7. Migration Strategy

    7.1. Backward Compatibility
        - Breaking tests, that deal with tokens must be updated to work on the new tokens, no background compatibility
		- This will invariably break token -> ast conversion, which must be updated on the old pipeline too .
		- Everyhting that happens after parsing is unaffected (AST Nodes)
        - AST structure remains the same (only token pipeline changes)
        - Tool integrations unaffected


    7.2. Implementation Order
        1. Update scanner token definitions
        2. Update old parser to use semantic tokens
        4. Update specifications

    7.3. Validation
        - All existing tests must pass
        - Token round-trip reconstruction must be perfect

8. Risk Mitigation

    8.1. Testing Strategy
        - Comprehensive token pipeline tests
        - Round-trip formatting preservation tests

    8.2. Rollback Plan
        - No Rollback