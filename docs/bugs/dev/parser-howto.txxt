Parser Implementation Guide

    This guide covers the implementation approach for the txxt parser, building on the comprehensive file structure established in Item 5.1.

**STRICT PARSER IMPLEMENTATION PROTOCOL - MUST FOLLOW EXACTLY**

**RULE 1: ONE ELEMENT AT A TIME - NO EXCEPTIONS**
- Implement exactly ONE element per commit
- Each element must have: code + tests + AST assertions + commit 
- Do NOT proceed to next element until current one is committed and pushed
- Do NOT create test files for unimplemented elements
- Respect file layout for parser, ast and tests with mirrored layouts.

**RULE 2: CORPORA SOURCES ONLY - NO AD HOC STRINGS**
- ALL test sources MUST come from corpora (docs/specs with :: txxt.core.spec.* labels)
- NEVER use inline strings like `r#"- First item"`
- If corpora doesn't exist for your element, CREATE it in the spec first
- Use TxxtCorpora::load_with_processing() for all parser tests

**RULE 3: MANDATORY FILE STRUCTURE - MIRROR SPECS EXACTLY**
- Parser code goes in src/parser/elements/[element].rs
- Tests go in tests/parser/elements/[element].rs (mirrors src structure)
- AST structures already exist in src/ast/ (follow existing structure)
- Source of truth is ALWAYS the spec in docs/specs/elements/[element].txxt

**RULE 4: IMPLEMENTATION SEQUENCE - MANDATORY ORDER**
2. Simple forms only: paragraphs, annotations, definitions, verbatim blocks, lists (step 1)
3. Advanced forms (step 2)
4. Containers without recursion (step 3.1)
5. [etc.]

**RULE 5: BEFORE STARTING ANY ELEMENT:**
1. Check if corpora exists: `:: txxt.core.spec.[element].valid.simple` 
2. If not, create it in docs/specs/elements/[element].txxt
3. Read the element specification completely
4. Identify what AST assertions are needed, create it .

**RULE 6: TESTING PROTOCOL:**
- Create single test file: tests/parser/elements/[element].rs
- Use TxxtCorpora for ALL test sources
- Use custom AST assertions for ALL test logic
- Test ONLY the current element, nothing else

**CHECKPOINT QUESTIONS TO ASK BEFORE PROCEEDING:**
- Am I implementing exactly one element?
- Am I using corpora sources (not ad hoc strings)?
- Do I have the corpora defined in the spec?
- Have I committed the previous element?
- Am I following the exact file structure?

1. File Structure Overview

    The parser implementation follows a three-phase architecture that mirrors the specification structure:

    src/parser/
    ├── pipeline/           # Three-phase processing pipeline
    │   ├── lexer.rs       # Phase 1: Tokenization wrapper
    │   ├── block_grouper.rs # Phase 2a: Block hierarchy
    │   ├── parser.rs      # Phase 2b: AST generation
    │   └── post_processor.rs # Phase 3: Final assembly
    ├── elements/          # Element-specific parsers (mirrors docs/specs/elements/)
    │   ├── paragraph.rs   # Basic text blocks
    │   ├── session.rs     # Hierarchical sections
    │   ├── container.rs   # Content containers
    │   ├── verbatim.rs    # Code and literal blocks
    │   ├── list.rs        # Ordered and unordered lists
    │   ├── definition.rs  # Definition pairs
    │   ├── annotation.rs  # Metadata attachments
    │   └── inlines/       # Inline element parsers
    ├── core/              # Shared parsing utilities
    │   ├── indentation.rs # Indentation handling
    │   ├── line_grouping.rs # Line organization
    │   └── span_utils.rs  # Token span management
    └── infrastructure/    # Error handling and validation

    src/ast/
    ├── base.rs           # Core AST types (Document, Block, etc.)
    ├── nodes/            # Element-specific AST nodes (mirrors docs/specs/elements/)
    │   ├── paragraph.rs  # Paragraph AST structures
    │   ├── session.rs    # Session AST structures
    │   ├── container.rs  # Container AST structures
    │   └── inlines/      # Inline AST nodes
    └── detokenizer.rs    # AST → source reconstruction

    tests/parser/
    ├── elements/          # Element tests (mirrors src/parser/elements/)
    │   ├── paragraph.rs   # Paragraph parsing tests
    │   ├── session.rs     # Session parsing tests  
    │   ├── container.rs   # Container parsing tests
    │   ├── verbatim.rs    # Verbatim parsing tests
    │   ├── list.rs        # List parsing tests
    │   ├── definition.rs  # Definition parsing tests
    │   ├── annotation.rs  # Annotation parsing tests
    │   └── inlines/       # Inline element tests
    └── integration/       # Cross-element integration tests

    The source of truth is ALWAYS the specification in docs/specs/elements/ and tests must follow the same file layout structure.


2. Source Of Truth

    The only source of truth is the specs: 
        docs/specs
            ├── core
            │   ├── common-processing.txxt
            │   ├── syntax.txxt
            │   └── terminology.txxt
            └── elements
                ├── annotation.txxt
                ├── container.txxt
                ├── definition.txxt
                ├── inlines
                │   ├── formatting.txxt
                │   ├── inlines-general.txxt
                │   └── references
                │       ├── citations.txxt
                │       └── references-general.txxt
                ├── labels.txxt
                ├── list.txxt
                ├── paragraph.txxt
                ├── parameters.txxt
                ├── session.txxt
                └── verbatim.txxt
    tree:: 

    For each element you work on it's key that you read docs/specs/core/syntax.txxt the element spec and if needed related ones (as parameters, labels and containters)


    Core Implementation Principles

    AST Specification Alignment

        Each AST node must precisely reflect its corresponding specification in docs/specs/elements/. The parser validates both syntactic structure and semantic constraints defined in the specs.

        Key requirements:
        - Token-level precision for source reconstruction
        - Parameter validation according to element specs
        - Proper handling of nested structures
        - Annotation attachment using proximity rules

3. Testing X

    3.a Where to get the txxt source
        
        If you feed invalid txxt sources to your tests, nothing will be correct.
        Hence it's key that you use only vettoed and verified txxt sources. These 
        are the ones in the specs, and the TxxtCorpora testing framework established in tests/corpora.rs and tests/parser_integration.rs. will give you one liners to leead them.

        Testing approach:
        - Use TxxtCorpora::load .
        - Implement tests for each processing stage: tokenization → block grouping → parsing → post-processing
        - Test both valid cases and error conditions

        Example test pattern:
            #[rstest]
            #[case::simple("paragraph-basic.txxt")]
            #[case::with_params("paragraph-params.txxt")]
            fn test_paragraph_parsing(#[case] corpus_file: &str) {
                let corpus = load_corpus(corpus_file);
                // Test parsing stages...
            }
        rust ::
    
    3.b  High level assertions. 

        Paring test tend to do much AST traversal, which is hard to write , read and britlle. 
        For each node you take you should use high level assertions, which are pplciation logic level as in node.assert_paragraph(line_count=3, text_contains"Hello)" and so on. 
        Each high level assertion should expose params that make testing easy.

        Also, this protects us from having britlte tests. Withouth doing direct ast traversal, changes to the ast can be fixed in one place, and not many


4. Implementation stages.

    The sequencing of parsing implementation is key. Being a recursive language, its critical that we can master the isolated documetn first before tackling the recursive versions.

    Inlines are much simpler, since they have no recursion, no indentation and have clear markers. For this reason, inlines will be done last, when all riskier code is working. 

    The general sequence is to implement: 
        0. Labels and parameters: other elemetns will use this.
        1. Simmple forms of paragraphs, annotations, definitions, verbatim blocks, lists.
        2. More advanced forms of these recursion not withstanding. 
        3. Content Container: 
            1. Without Recursion
            2. With Recusion
        4. Session Container:
            1. Withough Recursion
            2. with Recursion
        5. Structural edge cases
        6. Inlines
        
    1. Isolation and Flat
        
        The first round will be about elements in isolation, in their simple form.
        You will be given an element and the simple or advanced form.

        1. Read the spec for you element. 
        2. Choose test cases for the form you are working on .
        3. Tag them as per the corpora tool. 
        4. Write the tests.
        5. ATTENTION: you cannot write new txxt sources. If you feel the examples in the spec are not enough, do the work with what's there and then write your suggestion of new txxt source o include in the pr. 
        6. Isolation and 

    2. Content Containers: Flat

        You will now write the parsing for a content container. 

        1. Write an ensable of txxt sources.
        2. At first no nesting.
        3. Add one element per test, correct verify
        4. Until you are able to parse a content container will all nodes. (not sessions) nor inlines

   3. Content Containers: Nested 

        Now that you know you can parse a flat container, do the recursive parsing of content continers , which included nested lists, and all other content containers elemetnes (see spec)

   4. Session Contianer: Flat 
    
        Now you will imeplement a session continer, with all other elment types (but other sessions)

    5. Session Container: Nested

        Now you will do the recursive version and impelemnt recursive testing.

        When this is done, all the high complexity code is done, celebrate, and then 
        we can finilize with the inlines.

    6. Inlines

        Now you can work through inlines. These are much simpler as there is no ambuiguity, the syntax markers are defined explicitly , no indentation and no complicated recursion.

        6.1 Formatters : bold, italic, code, math
        6.2 References: url, sessions, footnotes, paths, tk, citations.

    The goal is systematic, specification-driven implementation that maintains the high quality established in the tokenizer phase.